} else {
for (i in seq(1, npages, 50)) { Sys.sleep(10)}
tmp <- GET(final.url)
tmp <- content(tmp, type = "text")
write(tmp, file.name)
page <- read_html(tmp)
}
titles <- page %>%
html_nodes(".csc-firstHeader") %>%
html_text(trim = T)
links <- page %>%
html_nodes('.news-item a') %>%
html_attr("href")
texts <- page %>%
html_nodes(".news-item") %>%
html_text(trim=T)
# plow data into dataframe
df <- data.frame(
title = titles[1],
# date = dates,
# byline = authors,
#  excerpt = excerpts,
link = paste(links[grepl("http", links)], collapse = ", "),
text = texts
)
# put page 'i' dataframe into the list
dfs[[i]] <- df
}
for (i in 1:npages) {
final.url <- url_list_fao[i]
#update progress bar
# setTxtProgressBar(pb, i)
# build file name
file.name <- paste0("./io_cooperation/fao_page", i, "---", ".html")
# avoid re-downloading by checking for files before downloading
if (file.exists(file.name)) {
page <- read_html(file.name)
} else {
if (i == 360 | i == 400)  Sys.sleep(10)
tmp <- GET(final.url)
tmp <- content(tmp, type = "text")
write(tmp, file.name)
page <- read_html(tmp)
}
titles <- page %>%
html_nodes(".csc-firstHeader") %>%
html_text(trim = T)
links <- page %>%
html_nodes('.news-item a') %>%
html_attr("href")
texts <- page %>%
html_nodes(".news-item") %>%
html_text(trim=T)
# plow data into dataframe
df <- data.frame(
title = titles[1],
# date = dates,
# byline = authors,
#  excerpt = excerpts,
link = paste(links[grepl("http", links)], collapse = ", "),
text = texts
)
# put page 'i' dataframe into the list
dfs[[i]] <- df
}
for (i in 1:npages) {
final.url <- url_list_fao[i]
#update progress bar
# setTxtProgressBar(pb, i)
# build file name
file.name <- paste0("./io_cooperation/fao_page", i, "---", ".html")
# avoid re-downloading by checking for files before downloading
if (file.exists(file.name)) {
page <- read_html(file.name)
} else {
if (i %% 2)  Sys.sleep(60) print("sleep")
tmp <- GET(final.url)
tmp <- content(tmp, type = "text")
write(tmp, file.name)
page <- read_html(tmp)
}
titles <- page %>%
html_nodes(".csc-firstHeader") %>%
html_text(trim = T)
links <- page %>%
html_nodes('.news-item a') %>%
html_attr("href")
texts <- page %>%
html_nodes(".news-item") %>%
html_text(trim=T)
# plow data into dataframe
df <- data.frame(
title = titles[1],
# date = dates,
# byline = authors,
#  excerpt = excerpts,
link = paste(links[grepl("http", links)], collapse = ", "),
text = texts
)
# put page 'i' dataframe into the list
dfs[[i]] <- df
}
if (i %% 2)
for (i in 1:npages) {
final.url <- url_list_fao[i]
#update progress bar
# setTxtProgressBar(pb, i)
# build file name
file.name <- paste0("./io_cooperation/fao_page", i, "---", ".html")
# avoid re-downloading by checking for files before downloading
if (file.exists(file.name)) {
page <- read_html(file.name)
} else {
if ((i %% 2) == 0)  Sys.sleep(60) print("sleep")
tmp <- GET(final.url)
tmp <- content(tmp, type = "text")
write(tmp, file.name)
page <- read_html(tmp)
}
titles <- page %>%
html_nodes(".csc-firstHeader") %>%
html_text(trim = T)
links <- page %>%
html_nodes('.news-item a') %>%
html_attr("href")
texts <- page %>%
html_nodes(".news-item") %>%
html_text(trim=T)
# plow data into dataframe
df <- data.frame(
title = titles[1],
# date = dates,
# byline = authors,
#  excerpt = excerpts,
link = paste(links[grepl("http", links)], collapse = ", "),
text = texts
)
# put page 'i' dataframe into the list
dfs[[i]] <- df
}
for (i in 1:npages) {
final.url <- url_list_fao[i]
#update progress bar
# setTxtProgressBar(pb, i)
# build file name
file.name <- paste0("./io_cooperation/fao_page", i, "---", ".html")
# avoid re-downloading by checking for files before downloading
if (file.exists(file.name)) {
page <- read_html(file.name)
} else {
if ((i %% 2) == 0)  Sys.sleep(60)
tmp <- GET(final.url)
tmp <- content(tmp, type = "text")
write(tmp, file.name)
page <- read_html(tmp)
}
titles <- page %>%
html_nodes(".csc-firstHeader") %>%
html_text(trim = T)
links <- page %>%
html_nodes('.news-item a') %>%
html_attr("href")
texts <- page %>%
html_nodes(".news-item") %>%
html_text(trim=T)
# plow data into dataframe
df <- data.frame(
title = titles[1],
# date = dates,
# byline = authors,
#  excerpt = excerpts,
link = paste(links[grepl("http", links)], collapse = ", "),
text = texts
)
# put page 'i' dataframe into the list
dfs[[i]] <- df
}
for (i in 1:npages) {
final.url <- url_list_fao[i]
#update progress bar
# setTxtProgressBar(pb, i)
# build file name
file.name <- paste0("./io_cooperation/fao_page", i, "---", ".html")
# avoid re-downloading by checking for files before downloading
if (file.exists(file.name)) {
page <- read_html(file.name)
} else {
if ((i %% 2) == 0)  Sys.sleep(60)
tmp <- GET(final.url)
tmp <- content(tmp, type = "text")
write(tmp, file.name)
page <- read_html(tmp)
}
titles <- page %>%
html_nodes(".csc-firstHeader") %>%
html_text(trim = T)
links <- page %>%
html_nodes('.news-item a') %>%
html_attr("href")
texts <- page %>%
html_nodes(".news-item") %>%
html_text(trim=T)
# plow data into dataframe
df <- data.frame(
title = titles[1],
# date = dates,
# byline = authors,
#  excerpt = excerpts,
link = paste(links[grepl("http", links)], collapse = ", "),
text = texts
)
# put page 'i' dataframe into the list
dfs[[i]] <- df
}
for (i in 1:npages) {
final.url <- url_list_fao[i]
#update progress bar
# setTxtProgressBar(pb, i)
# build file name
file.name <- paste0("./io_cooperation/fao_page", i, "---", ".html")
# avoid re-downloading by checking for files before downloading
if (file.exists(file.name)) {
page <- read_html(file.name)
} else {
if ((i %% 2) == 0)  Sys.sleep(180)
tmp <- GET(final.url)
tmp <- content(tmp, type = "text")
write(tmp, file.name)
page <- read_html(tmp)
}
titles <- page %>%
html_nodes(".csc-firstHeader") %>%
html_text(trim = T)
links <- page %>%
html_nodes('.news-item a') %>%
html_attr("href")
texts <- page %>%
html_nodes(".news-item") %>%
html_text(trim=T)
# plow data into dataframe
df <- data.frame(
title = titles[1],
# date = dates,
# byline = authors,
#  excerpt = excerpts,
link = paste(links[grepl("http", links)], collapse = ", "),
text = texts
)
# put page 'i' dataframe into the list
dfs[[i]] <- df
}
final.url
for (i in 1:npages) {
final.url <- url_list_fao[i]
#update progress bar
# setTxtProgressBar(pb, i)
# build file name
file.name <- paste0("./io_cooperation/fao_page", i, "---", ".html")
# avoid re-downloading by checking for files before downloading
if (file.exists(file.name)) {
page <- read_html(file.name)
} else {
if ((i %% 2) == 0)  Sys.sleep(180)
tmp <- GET(final.url)
tmp <- content(tmp, type = "text")
write(tmp, file.name)
page <- read_html(tmp)
}
titles <- page %>%
html_nodes(".csc-firstHeader") %>%
html_text(trim = T)
links <- page %>%
html_nodes('.news-item a') %>%
html_attr("href")
texts <- page %>%
html_nodes(".news-item") %>%
html_text(trim=T)
# plow data into dataframe
df <- data.frame(
title = titles[1],
# date = dates,
# byline = authors,
#  excerpt = excerpts,
link = paste(links[grepl("http", links)], collapse = ", "),
text = texts
)
# put page 'i' dataframe into the list
dfs[[i]] <- df
}
for (i in 1:npages) {
final.url <- url_list_fao[i]
#update progress bar
# setTxtProgressBar(pb, i)
# build file name
file.name <- paste0("./io_cooperation/fao_page", i, "---", ".html")
# avoid re-downloading by checking for files before downloading
if (file.exists(file.name)) {
page <- read_html(file.name)
} else {
if ((i %% 2) == 0)  Sys.sleep(180)
tmp <- GET(final.url)
tmp <- content(tmp, type = "text")
write(tmp, file.name)
page <- read_html(tmp)
}
titles <- page %>%
html_nodes(".csc-firstHeader") %>%
html_text(trim = T)
links <- page %>%
html_nodes('.news-item a') %>%
html_attr("href")
texts <- page %>%
html_nodes(".news-item") %>%
html_text(trim=T)
# plow data into dataframe
df <- data.frame(
title = titles[1],
# date = dates,
# byline = authors,
#  excerpt = excerpts,
link = paste(links[grepl("http", links)], collapse = ", "),
text = texts
)
# put page 'i' dataframe into the list
dfs[[i]] <- df
}
final.url
# avoid re-downloading by checking for files before downloading
if (file.exists(file.name)) {
page <- read_html(file.name)
} else {
#  if ((i %% 2) == 0)  Sys.sleep(180)
tmp <- GET(final.url)
tmp <- content(tmp, type = "text")
write(tmp, file.name)
page <- read_html(tmp)
}
for (i in 1:npages) {
final.url <- url_list_fao[i]
#update progress bar
# setTxtProgressBar(pb, i)
# build file name
file.name <- paste0("./io_cooperation/fao_page", i, "---", ".html")
# avoid re-downloading by checking for files before downloading
if (file.exists(file.name)) {
page <- read_html(file.name)
} else {
#  if ((i %% 2) == 0)  Sys.sleep(180)
tmp <- GET(final.url)
tmp <- content(tmp, type = "text")
write(tmp, file.name)
page <- read_html(tmp)
}
titles <- page %>%
html_nodes(".csc-firstHeader") %>%
html_text(trim = T)
links <- page %>%
html_nodes('.news-item a') %>%
html_attr("href")
texts <- page %>%
html_nodes(".news-item") %>%
html_text(trim=T)
# plow data into dataframe
df <- data.frame(
title = titles[1],
# date = dates,
# byline = authors,
#  excerpt = excerpts,
link = paste(links[grepl("http", links)], collapse = ", "),
text = texts
)
# put page 'i' dataframe into the list
dfs[[i]] <- df
}
for (i in 1:npages) {
final.url <- url_list_fao[i]
#update progress bar
# setTxtProgressBar(pb, i)
# build file name
file.name <- paste0("./io_cooperation/fao_page", i, "---", ".html")
# avoid re-downloading by checking for files before downloading
if (file.exists(file.name)) {
page <- read_html(file.name)
} else {
#  if ((i %% 2) == 0)  Sys.sleep(180)
tmp <- GET(final.url)
tmp <- content(tmp, type = "text")
write(tmp, file.name)
page <- read_html(tmp)
}
titles <- page %>%
html_nodes(".csc-firstHeader") %>%
html_text(trim = T)
links <- page %>%
html_nodes('.news-item a') %>%
html_attr("href")
texts <- page %>%
html_nodes(".news-item") %>%
html_text(trim=T)
# plow data into dataframe
df <- data.frame(
title = titles[1],
# date = dates,
# byline = authors,
#  excerpt = excerpts,
link = paste(links[grepl("http", links)], collapse = ", "),
text = texts
)
# put page 'i' dataframe into the list
dfs[[i]] <- df
}
library(igraph)
igo <-       str_to_lower(c("NEAFC", "ISA",
"HELCOM", "CPPS", "NAFO",
"NPFC","OSPAR", "SPREP", "WCPFC", "AALCO",
"ICES", "Commonwealth", "Iccat", "SPC", "IPBES", "IATTC", "Medfund", "GCF", "SEAFO",
"Pacific Island Forum", "SICA", "WTO", "Nauru Agreement", "ILO", "IWC", "ATS",
"Benguela", "PEMSEA", "IPCC", "CCAMLR", "APFIC",
"GLFC", "IOTC", "IPHC", "NASCO", "PSC",
"SEAFDEC", "NACA", "PICES", "IAEA", "WMO", "world bank","UNIDO", "UNSD", "UNCLOS", "UNDESA",
"UNECE", "UNFCCC", "UNEP", "UNDP", "UNFF", "Commission on the limits of the continental shelf",
"ITLOS", "GESAMP", "CMS", "Abidjan", "Minamata","CBD", "cites", "Barcelona"))
#################### IOC
ioc <- read.csv("out_ioc.csv")
DT <- as.data.table(ioc)
#text
x <- c()
for (i in igo){ x = c(x, igo = length(grep(i, DT$text)))}
df_ioc1 <- data.frame(ioc1 = matrix(unlist(x), nrow=length(x), byrow=T))
#links
x <- c()
for (i in igo){ x = c(x, igo = length(grep(i, DT$link)))}
df_ioc2 <- data.frame(ioc2 = matrix(unlist(x), nrow=length(x), byrow=T))
df_ioc <- cbind(df_ioc1, df_ioc2)
df_ioc <- df_ioc %>% mutate(ioc = ioc1 + ioc2) %>% select(ioc)
row.names(df_ioc) <- igo
########################### ISA
isa <- read.csv("out_isa.csv")
DT <- as.data.table(isa)
# text
x <- c()
for (i in igo){ x = c(x, igo = length(grep(i, DT$text)))}
df_isa1 <- data.frame(isa1 = matrix(unlist(x), nrow=length(x), byrow=T))
# link
x <- c()
for (i in igo){ x = c(x, igo = length(grep(i, DT$link)))}
df_isa2 <- data.frame(isa2 = matrix(unlist(x), nrow=length(x), byrow=T))
df_isa <- cbind(df_isa1, df_isa2)
df_isa <- df_isa %>% mutate(isa = isa1 + isa2) %>% select(isa)
row.names(df_isa) <- igo
######################## UNEP
unep <- read.csv("out_unep.csv")
DT <- as.data.table(unep)
# text
x <- c()
for (i in igo){ x = c(x, igo = length(grep(i, DT$text)))}
df_unep1 <- data.frame(unep1 = matrix(unlist(x), nrow=length(x), byrow=T))
# links
x <- c()
for (i in igo){ x = c(x, igo = length(grep(i, DT$link)))}
df_unep2 <- data.frame(unep2 = matrix(unlist(x), nrow=length(x), byrow=T))
df_unep <- cbind(df_unep1, df_unep2)
df_unep <- df_unep %>% mutate(unep = unep1 + unep2) %>% select(unep)
row.names(df_unep) <- igo
######################## cbd
cbd <- read.csv("out_cbd.csv")
DT <- as.data.table(cbd)
# text
x <- c()
for (i in igo){ x = c(x, igo = length(grep(i, DT$text)))}
df_cbd1 <- data.frame(cbd1 = matrix(unlist(x), nrow=length(x), byrow=T))
# links
x <- c()
for (i in igo){ x = c(x, igo = length(grep(i, DT$link)))}
df_cbd2 <- data.frame(cbd2 = matrix(unlist(x), nrow=length(x), byrow=T))
df_cbd <- cbind(df_cbd1, df_cbd2)
df_cbd <- df_cbd %>% mutate(cbd = cbd1 + cbd2) %>% select(cbd)
row.names(df_cbd) <- igo
######################## imo
imo <- read.csv("out_imo.csv")
DT <- as.data.table(imo)
# text
x <- c()
for (i in igo){ x = c(x, igo = length(grep(i, DT$text)))}
df_imo1 <- data.frame(imo1 = matrix(unlist(x), nrow=length(x), byrow=T))
# links
x <- c()
for (i in igo){ x = c(x, igo = length(grep(i, DT$link)))}
df_imo2 <- data.frame(imo2 = matrix(unlist(x), nrow=length(x), byrow=T))
df_imo <- cbind(df_imo1, df_imo2)
df_imo <- df_imo %>% mutate(imo = imo1 + imo2) %>% select(imo)
row.names(df_imo) <- igo
######################## fao
fao <- read.csv("out_fao.csv")
DT <- as.data.table(fao)
# text
x <- c()
for (i in igo){ x = c(x, igo = length(grep(i, DT$text)))}
df_fao1 <- data.frame(fao1 = matrix(unlist(x), nrow=length(x), byrow=T))
# links
x <- c()
for (i in igo){ x = c(x, igo = length(grep(i, DT$link)))}
df_fao2 <- data.frame(fao2 = matrix(unlist(x), nrow=length(x), byrow=T))
df_fao <- cbind(df_fao1, df_fao2)
df_fao <- df_fao %>% mutate(fao = fao1 + fao2) %>% select(fao)
row.names(df_fao) <- igo
inc_net <- cbind(df_ioc, df_isa, df_unep, df_fao, df_imo, df_cbd)
View(inc_net)
i
paste0(\b i \b)
paste0("\b i \b")
DT$text
